{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dd07fac-3f67-4832-ae1c-f0785eb9b973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 23:13:40.272400: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import pandas as pd\n",
    "import skrf as rf\n",
    "\n",
    "#define the targeted max frequency\n",
    "fmax = 196\n",
    "#define function for loading images and s-parameters\n",
    "#\"path\" is the path for images of transformer layout\n",
    "#\"label\" is the path for labels. In this case, labels are S-parameter associated with each images \n",
    "def load_images_from_path(path, label,length):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(length):\n",
    "        #load images\n",
    "        img = image.load_img(path+str(i)+'.png', target_size=(500, 500, 3))\n",
    "        images.append(image.img_to_array(img)[138:362,138:362,:])#trim the ground margin\n",
    "\n",
    "        #load S-parameters\n",
    "        #According to symmerty, only S11,S12,S13,S14,S33,S34 are considered\n",
    "        #Each S-parameter has real and imaginary parts, and hence 12 real values in total.\n",
    "        results = []\n",
    "        datafile = label+str(i)+\".s4p\"\n",
    "        spt = rf.Network(datafile)\n",
    "        for i in range(fmax+1):\n",
    "            results.append([spt.s[i][0][0].real,spt.s[i][0][0].imag,\n",
    "                            spt.s[i][0][1].real,spt.s[i][0][1].imag,\n",
    "                            spt.s[i][0][2].real,spt.s[i][0][2].imag,\n",
    "                            spt.s[i][0][3].real,spt.s[i][0][3].imag,\n",
    "                            spt.s[i][2][2].real,spt.s[i][2][2].imag,\n",
    "                            spt.s[i][2][3].real,spt.s[i][2][3].imag])      \n",
    "        labels.append(results) \n",
    "    return images, labels\n",
    "\n",
    "def show_images(images):\n",
    "    fig, axes = plt.subplots(1, 8, figsize=(20, 20), subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i] / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af224110-3872-4743-a9ec-17c654d5d66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48550f-538f-48a5-bf64-dc3d154961bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "#geometrical parameters stored in log.txt\n",
    "#order of geopara (radiusA,radiusB,turnsA,turnsB,openA,openB,outA,outB,extA,extB,ratio,outbound)\n",
    "logfile = open(\"/rdf/shared/design_automation/Data/2409Air/log.txt\",\"r\")\n",
    "listall = list(map(lambda item: list(map(lambda jtem: eval(jtem.strip()), item[1:-1].split(','))), logfile.read().strip().split('\\n')))\n",
    "tabular_data = np.array(listall)\n",
    "\n",
    "#load images and s-parameters\n",
    "length = 4000\n",
    "images, labels = load_images_from_path('/rdf/shared/design_automation/Data/2409Air/PNG/', '/rdf/shared/design_automation/Data/2409Air/SPData/',length)\n",
    "show_images(images)\n",
    "geoparas = []\n",
    "images11 = []\n",
    "labels11 = []\n",
    "\n",
    "for kk in range(length):\n",
    "    #select 1 turn and 1 turn transformers\n",
    "   if((tabular_data[kk,2]==1) and (tabular_data[kk,3]==1)):\n",
    "       #geoparas only includes the variable parameters in our designs. \n",
    "       #Constants at this design stages are excluded \n",
    "        geoparas.append(tabular_data[kk,(0,1,4,5,4,5,6,7,12,13)])\n",
    "        images11.append(images[kk])\n",
    "        labels11.append(labels[kk])\n",
    "print(np.shape(geoparas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d169da-3418-43ff-b03a-d32b565594d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from the second folder\n",
    "#geometrical parameters stored in log.txt\n",
    "#order of geopara (radiusA,radiusB,turnsA,turnsB,openA,openB,outA,outB,extA,extB,ratio,outbound)\n",
    "logfile = open(\"/rdf/shared/design_automation/Data/2409Air_2/log.txt\",\"r\")\n",
    "listall = list(map(lambda item: list(map(lambda jtem: eval(jtem.strip()), item[1:-1].split(','))), logfile.read().strip().split('\\n')))\n",
    "tabular_data = np.array(listall)\n",
    "\n",
    "#load images and s-parameters\n",
    "#length = len(listall)-3\n",
    "length = 900\n",
    "images, labels = load_images_from_path('/rdf/shared/design_automation/Data/2409Air_2/PNG/', '/rdf/shared/design_automation/Data/2409Air_2/SPData/',length)\n",
    "\n",
    "\n",
    "for kk in range(length):\n",
    "    #select 1 turn and 1 turn transformers\n",
    "   if((tabular_data[kk,2]==1) and (tabular_data[kk,3]==1)):\n",
    "       #geoparas only includes the variable parameters in our designs. \n",
    "       #Constants at this design stages are excluded \n",
    "        geoparas.append(tabular_data[kk,(0,1,4,5,4,5,6,7,12,13)])\n",
    "        images11.append(images[kk])\n",
    "        labels11.append(labels[kk])\n",
    "print(np.shape(geoparas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f8ba0-1c01-4b50-a89c-9973af709eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize geometrical inputs\n",
    "geoparas_temp = np.array(geoparas)\n",
    "geoparas = np.zeros(np.shape(geoparas_temp))\n",
    "for i in range(len(geoparas)):\n",
    "    for j in range(10):\n",
    "        geoparas[i,j]= (geoparas_temp[i,j]-np.mean(geoparas_temp[:,j]))/np.std(geoparas_temp[:,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13975a-1b1b-4d06-bff4-1ba88982a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct the datasets\n",
    "class item:\n",
    "  def __init__(self, image,geopara,label ):\n",
    "    self.image = image\n",
    "    self.geopara = geopara\n",
    "    self.label = label\n",
    "\n",
    "data = []\n",
    "for i in range(len(images11)):\n",
    "    data.append(item(images11[i],geoparas[i],labels11[i]))\n",
    "\n",
    "#np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876b932-eee7-46d4-9269-b9268a591a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the split length for training, validation and test datasets\n",
    "length = len(images11)\n",
    "split1 = int(0.6*length)\n",
    "split2 = int(0.8*length)\n",
    "data_train = data[0:split1]\n",
    "data_valid = data[split1:split2]\n",
    "data_test = data[split2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f9fbb-5ad2-44e6-865c-beb82eedc475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate the training, validation and test datasets \n",
    "train_images = []\n",
    "train_labels = []\n",
    "train_geoparas = []\n",
    "valid_images = []\n",
    "valid_labels = []\n",
    "valid_geoparas = []\n",
    "test_images = []\n",
    "test_labels = []\n",
    "test_geoparas = []\n",
    "\n",
    "\n",
    "#The targeted frequency span up to fmax, with 1GHz step\n",
    "s_max = np.zeros((fmax,12))\n",
    "s_min = np.zeros((fmax,12))\n",
    "srange = np.zeros((fmax,12))\n",
    "\n",
    "\n",
    "for i in range(len(data_train)):\n",
    "    train_images.append(data_train[i].image)\n",
    "    train_geoparas.append(data_train[i].geopara)\n",
    "    train_labels.append(data_train[i].label)\n",
    "x_train_img = np.array(train_images) / 255\n",
    "x_train_geopara = np.array(train_geoparas)\n",
    "y_train_encoded = np.array(train_labels)[:,1:fmax+1,:]\n",
    "\n",
    "\n",
    "for i in range(len(data_valid)):\n",
    "    valid_images.append(data_valid[i].image)\n",
    "    valid_geoparas.append(data_valid[i].geopara)\n",
    "    valid_labels.append(data_valid[i].label)\n",
    "x_valid_img = np.array(valid_images) / 255\n",
    "x_valid_geopara = np.array(valid_geoparas)\n",
    "y_valid_encoded = np.array(valid_labels)[:,1:fmax+1,:]\n",
    "\n",
    "\n",
    "for i in range(len(data_test)):\n",
    "    test_images.append(data_test[i].image)\n",
    "    test_geoparas.append(data_test[i].geopara)\n",
    "    test_labels.append(data_test[i].label)\n",
    "x_test_img = np.array(test_images) / 255\n",
    "x_test_geopara = np.array(test_geoparas)\n",
    "y_test_encoded = np.array(test_labels)[:,1:fmax+1,:]\n",
    "\n",
    "#normalize the s-parameters for each frequency point\n",
    "for z in range(fmax):\n",
    "    for i in range(12):\n",
    "        s_max[z,i] = max(max(y_train_encoded[:,z,i]),max(y_valid_encoded[:,z,i]),max(y_test_encoded[:,z,i]))\n",
    "        s_min[z,i] = min(min(y_train_encoded[:,z,i]),min(y_valid_encoded[:,z,i]),min(y_test_encoded[:,z,i]))\n",
    "        srange[z,i] = s_max[z,i]-s_min[z,i]\n",
    "    \n",
    "    for i in range(12):\n",
    "        y_train_encoded[:,z,i] = 2*(y_train_encoded[:,z,i]-s_min[z,i])/srange[z,i]-1\n",
    "\n",
    "    for i in range(12):\n",
    "        y_valid_encoded[:,z,i] = 2*(y_valid_encoded[:,z,i]-s_min[z,i])/srange[z,i]-1\n",
    "    \n",
    "    for i in range(12):\n",
    "        y_test_encoded[:,z,i] = 2*(y_test_encoded[:,z,i]-s_min[z,i])/srange[z,i]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f58e9-86a0-4314-9aae-5ce946b61965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    Lf = tf.math.sqrt(tf.reduce_mean(tf.square(y_true - y_pred),axis=1))\n",
    "#    return tf.math.log(tf.reduce_mean(Lf))\n",
    "    return (tf.reduce_mean(Lf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e8170-3327-404c-beaa-94062d1d0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential, Input, Model, regularizers\n",
    "from keras.layers import Conv2D, MaxPooling2D,Dropout, Reshape, BatchNormalization,Conv1DTranspose,Conv1D,Add\n",
    "from keras.layers import Flatten, Dense, Concatenate\n",
    "learning_rate = 1e-4\n",
    "n_unet = 8\n",
    "\n",
    "models = []\n",
    "\n",
    "inp1 = Input((224,224,3),)\n",
    "'''\n",
    "model1 = (Conv2D(n_unet,(3,3),padding='same', activation='relu', input_shape=(224, 224, 3), kernel_regularizer=regularizers.l2(0.01)))(inp1)\n",
    "model1 = (MaxPooling2D(2, 2))(model1)\n",
    "model1 = (Conv2D(n_unet*2, (3, 3), padding='same',activation='relu'))(model1)\n",
    "\n",
    "model1 = (MaxPooling2D(2, 2))(model1)\n",
    "#model1 = BatchNormalization()(model1)\n",
    "model1 = (Conv2D(n_unet*2, (3, 3),  padding='same',activation='relu'))(model1)\n",
    "\n",
    "model1 = (MaxPooling2D(2, 2))(model1)\n",
    "#model1 = BatchNormalization()(model1)\n",
    "model1 = (Conv2D(n_unet*4, (3, 3), padding='same', activation='relu'))(model1)\n",
    "\n",
    "layer1 = (MaxPooling2D(2, 2))(model1)\n",
    "#model1 = Dropout(0.3)(model1)\n",
    "#model1 = BatchNormalization()(model1)\n",
    "layer2 = (Conv2D(n_unet*8, (3, 3),padding='same', activation='relu'))(layer1)\n",
    "layer2 = (MaxPooling2D(2, 2))(layer2)\n",
    "\n",
    "layer3 = (Conv2D(n_unet*16, (3, 3),padding='same', activation='relu'))(layer2)\n",
    "layer3 = (MaxPooling2D(2, 2))(layer3)\n",
    "\n",
    "layer4 = (Conv2D(n_unet*32, (3, 3),padding='same', activation='relu'))(layer3)\n",
    "layer4 = (MaxPooling2D(2, 2))(layer4)\n",
    "\n",
    "\n",
    "#backward\n",
    "layer4 = Reshape((1,n_unet*32))(layer4)\n",
    "layer4 = (Conv1DTranspose(n_unet*16,9,1, activation='relu'))(layer4)\n",
    "\n",
    "layer3 = Reshape((9,n_unet*16))(layer3)\n",
    "layer3 = Concatenate()([layer3,layer4])\n",
    "layer3 = (Conv1D(n_unet*16,3,padding='same', activation='relu'))(layer3)\n",
    "layer3 = (Conv1DTranspose(n_unet*8,9,5, activation='relu'))(layer3)\n",
    "\n",
    "\n",
    "layer2 = Reshape((49,n_unet*8))(layer2)\n",
    "layer2 = Concatenate()([layer2,layer3])\n",
    "layer2 = (Conv1D(n_unet*8,3,padding='same', activation='relu'))(layer2)\n",
    "layer2 = (Conv1DTranspose(n_unet*4,3,4, activation='relu'))(layer2)\n",
    "\n",
    "layer1 = Reshape((196,n_unet*4))(layer1)\n",
    "layer1 = Concatenate()([layer1,layer2])\n",
    "layer1 = (Conv1D(n_unet*4,3,padding='same', activation='relu'))(layer2)\n",
    "model1 = Flatten()(layer1)\n",
    "models.append(model1)\n",
    "'''\n",
    "inp2 = Input(10,)\n",
    "\n",
    "model2=(Dense(512,activation='relu',input_shape = (10,),kernel_regularizer=regularizers.l2(0.10)))(inp2)\n",
    "\n",
    "model2=(Dense(512,activation='relu'))(model2)\n",
    "#skip = model2\n",
    "model2=(Dense(512,activation='relu'))(model2)\n",
    "model2=(Dense(512,activation='relu'))(model2)\n",
    "model2=(Dense(512,activation='relu'))(model2)\n",
    "#model2 = Add()([skip,model2])\n",
    "models.append(model2)\n",
    "'''\n",
    "model3 = (Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3), kernel_regularizer=regularizers.l2(0.05)))(inp1)\n",
    "model3 = (MaxPooling2D(2, 2))(model3)\n",
    "model3 = (Conv2D(128, (3, 3), activation='relu'))(model3)\n",
    "\n",
    "model3 = (MaxPooling2D(2, 2))(model3)\n",
    "#model1 = BatchNormalization()(model1)\n",
    "model3 = (Conv2D(128, (3, 3), activation='relu'))(model3)\n",
    "\n",
    "model3 = (MaxPooling2D(2, 2))(model3)\n",
    "#model1 = BatchNormalization()(model1)\n",
    "model3 = (Conv2D(128, (3, 3), activation='relu'))(model3)\n",
    "\n",
    "model3 = (MaxPooling2D(2, 2))(model3)\n",
    "#model1 = Dropout(0.3)(model1)\n",
    "#model1 = BatchNormalization()(model1)\n",
    "model3 = (Conv2D(128, (3, 3), activation='relu'))(model3)\n",
    "\n",
    "model3 = (MaxPooling2D(2, 2))(model3)\n",
    "#model1 = BatchNormalization()(model1)\n",
    "#model1 = Dropout(0.3)(model1)\n",
    "model3 = Flatten()(model3)\n",
    "model3 = Dense(512,activation='relu')(model3)\n",
    "\n",
    "models.append(model3)\n",
    "'''\n",
    "out = Concatenate()(models)\n",
    "\n",
    "out = (Dense(fmax*12, activation='tanh'))(out)\n",
    "out = Reshape((fmax,12))(out)\n",
    "model = Model([inp1,inp2],out)\n",
    "model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=learning_rate), loss=custom_loss, metrics=['mae',custom_loss])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306e60ae-cb7c-43dc-bad1-84b31a95cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: learning_rate * 0.95**(epoch/20))\n",
    "hist = model.fit([x_train_img,x_train_geopara], y_train_encoded, validation_data=([x_valid_img,x_valid_geopara], y_valid_encoded), batch_size=30, epochs = EPOCHS,callbacks=[lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dc1b1b-9f1b-4db0-9d0e-5998268b6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_train_encoded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbdcd45-2f92-4d93-a7de-086ac9e0cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = model.predict([x_test_img,x_test_geopara])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08adf5-b14c-4923-84dc-e3dc1ea68a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c568ed8-114d-4a4c-90e8-75a349108d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = hist.history['mae']\n",
    "val_mae = hist.history['val_mae']\n",
    "epochs = range(1, len(mae) + 1)\n",
    "\n",
    "plt.plot(epochs, mae, '-', label='Training mae')\n",
    "plt.plot(epochs, val_mae, ':', label='Validation mae')\n",
    "plt.title('Training and Validation mae')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('mae')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa100b8f-57fe-4ebc-a3bc-34e4ced83b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_result = []\n",
    "for z in range(fmax):\n",
    "    r2 = 0\n",
    "    for i in range(12):\n",
    "        temp = r2_score(y_test_encoded[:,z,i], test_prediction[:,z,i], force_finite=False)\n",
    "        r2 = r2+temp\n",
    "    r2_result.append(r2/12)\n",
    "plt.plot(range(fmax),r2_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5e477-cb3e-4e59-9305-89232ea0d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(r2_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d2451-fbe7-42ca-abde-e9b6c06d7cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_all = np.zeros((fmax,12))\n",
    "mae_mean = np.zeros(fmax)\n",
    "for z in range(fmax):\n",
    "    for i in range(12):\n",
    "        mae_all[z][i] = np.mean(abs(test_prediction[:,z,i]-y_test_encoded[:,z,i]))*0.5*srange[z,i]\n",
    "    mae_mean[z] = np.mean(mae_all[z][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67bfb44-9315-4ec5-b4b3-f1d8b398514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = range(fmax)\n",
    "for i in range(12):\n",
    "    if (i%2==0):\n",
    "        plt.plot(freq,mae_all[:,i])\n",
    "plt.legend([\"Re[S11]\",\"Re[S12]\",\"Re[S13]\",\"Re[S14]\",\"Re[S33]\",\"Re[S34]\"])\n",
    "plt.xlabel('Frequency/GHz')\n",
    "plt.ylabel('S-parameter mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ab6b7-fa7f-46ea-b940-ae280a7facf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmax = 196\n",
    "mae_all = np.zeros((fmax,12))\n",
    "mae_mean = np.zeros(fmax)\n",
    "for z in range(fmax):\n",
    "    for i in range(12):\n",
    "        mae_all[z][i] = np.mean(abs(test_prediction[:,z,i]-y_test_encoded[:,z,i]))*0.5*srange[z,i]\n",
    "    mae_mean[z] = np.mean(mae_all[z][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae32c1b-fa36-482f-8f87-9e88d699438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(fmax),mae_mean)\n",
    "plt.xlabel('Frequency/GHz')\n",
    "plt.ylabel('Average S-parameter mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74776122-26c2-4b34-9980-6b0abd7e50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = range(fmax)\n",
    "for i in range(12):\n",
    "    if (i%2==1):\n",
    "        plt.plot(freq,mae_all[:,i])\n",
    "plt.legend([\"Im[S11]\",\"Im[S12]\",\"Im[S13]\",\"Im[S14]\",\"Im[S33]\",\"Im[S34]\"])        \n",
    "plt.xlabel('Frequency/GHz')\n",
    "plt.ylabel('S-parameter mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f957f021-0e82-4af7-957b-843410c32c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mae_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab2f2c-c31d-470f-8636-de4875965701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_squared(y, y_pred):\n",
    "  residual = tf.reduce_sum(tf.square(tf.subtract(y, y_pred)))\n",
    "  total = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n",
    "  r2 = tf.subtract(1.0, tf.divide(residual, total))\n",
    "  return r2\n",
    "print(R_squared(y_test_encoded,test_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44918a-7cab-4690-bf68-0ffbe8a78844",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/air/1turn_196G_MLP.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aebddf0-0a82-4121-8f07-4189e21c32b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
